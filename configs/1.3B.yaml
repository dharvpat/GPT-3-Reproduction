model_size: "1.3B"
hidden_size: 2048
num_attention_heads: 32
num_layers: 24
ffn_hidden_size: 8192
dropout: 0.1
vocab_size: 50257
max_position_embeddings: 1024
learning_rate: 1e-4
weight_decay: 0.1
warmup_steps: 1000
scheduler: "linear"