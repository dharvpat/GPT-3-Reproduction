model_size: "64M"
hidden_size: 512  # Reduced hidden size to reduce parameters
num_attention_heads: 8  # Fewer attention heads to match the smaller hidden size
num_layers: 10  # Reduced number of layers
ffn_hidden_size: 2048  # Feed-forward network hidden size (typically 4x hidden_size)
dropout: 0.1
vocab_size: 50257
max_position_embeddings: 1024
learning_rate: 1e-4
weight_decay: 0.1
warmup_steps: 1000
scheduler: "linear"
optimizer: "adamw"
epochs: 5
embedding_dim: 512  # Adjust embedding dim to match hidden size